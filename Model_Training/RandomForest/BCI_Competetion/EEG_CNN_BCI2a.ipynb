{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6f396e",
   "metadata": {},
   "source": [
    "# EEG BCI Competition 2a — CNN baseline (trial-level)\n",
    "This notebook trains a simple 1D CNN on trial-level EEG time-series.\n",
    "\n",
    "**Key goal:** use the *same split procedure* as your teammate’s RandomForest notebook: `train_test_split(..., test_size=0.2, random_state=42, stratify=y)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60444c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U numpy pandas scikit-learn torch scipy mne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e00fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from mne.filter import filter_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "578f0acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Config (EDIT ME)\n",
    "# =====================\n",
    "DATA_DIR = r\"C:\\Users\\daiva\\Desktop\\Applied-Machine-Learning-Project\\Dataset\\BCI Competition 2a\\Trials\"\n",
    "# By default we read all 9 subject files like A01T_trials.csv ... A09T_trials.csv\n",
    "CSV_GLOB = os.path.join(DATA_DIR, \"A??T_trials.csv\")\n",
    "\n",
    "SUBJECTS = [\"A02T\"]\n",
    "#SUBJECTS = None  # None = use all subjects found\n",
    "\n",
    "# Split settings (MATCHES other models)\n",
    "TEST_SIZE = 0.2\n",
    "SPLIT_SEED = 42\n",
    "USE_STRATIFY = True\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "729ab88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSVs:\n",
      "  - A01T_trials.csv\n",
      "  - A02T_trials.csv\n",
      "  - A03T_trials.csv\n",
      "  - A04T_trials.csv\n",
      "  - A05T_trials.csv\n",
      "  - A06T_trials.csv\n",
      "  - A07T_trials.csv\n",
      "  - A08T_trials.csv\n",
      "  - A09T_trials.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (288, 22, 1000) y shape: (288,)\n",
      "Labels: [1, 2, 3, 4] -> classes: 4\n",
      "Saved preprocessed_trials.npz\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Data loading (same trial construction as teammate)\n",
    "#   - EEG columns = columns starting with 'EEG'\n",
    "#   - Build X as (n_trials, n_channels, n_times)\n",
    "#   - y = one label per trial\n",
    "# =====================\n",
    "\n",
    "def load_trials_from_csv(csv_path: str):\n",
    "    # read header to get EEG columns\n",
    "    header = pd.read_csv(csv_path, nrows=0)\n",
    "    eeg_cols = [c for c in header.columns if c.startswith('EEG')]\n",
    "    usecols = ['Trial_ID', 'Label', 'Subject'] + eeg_cols\n",
    "    df = pd.read_csv(csv_path, usecols=usecols)\n",
    "\n",
    "    # other notebooks use sorted unique trial IDs\n",
    "    trial_ids = sorted(df['Trial_ID'].unique())\n",
    "    n_times = (df[df['Trial_ID'] == trial_ids[0]].shape[0])\n",
    "    n_channels = len(eeg_cols)\n",
    "\n",
    "    X = np.zeros((len(trial_ids), n_channels, n_times), dtype=np.float32)\n",
    "    y = np.zeros((len(trial_ids),), dtype=np.int64)\n",
    "    trial_keys = []\n",
    "\n",
    "    for i, tid in enumerate(trial_ids):\n",
    "        trial = df[df['Trial_ID'] == tid]\n",
    "        # (n_times, n_channels) -> transpose to (n_channels, n_times)\n",
    "        X[i] = trial[eeg_cols].to_numpy(dtype=np.float32).T\n",
    "        y[i] = int(trial['Label'].iloc[0])\n",
    "        subj = str(trial['Subject'].iloc[0])\n",
    "        trial_keys.append(f\"{subj}_{int(tid)}\")\n",
    "\n",
    "    return X, y, trial_keys, eeg_cols\n",
    "\n",
    "csv_files = sorted(glob.glob(CSV_GLOB))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files matched {CSV_GLOB}. Check DATA_DIR.\")\n",
    "print(\"Found CSVs:\", *[os.path.basename(p) for p in csv_files], sep='\\n  - ')\n",
    "\n",
    "Xs, ys, keys = [], [], []\n",
    "eeg_cols_ref = None\n",
    "for p in csv_files:\n",
    "    Xp, yp, kp, eeg_cols = load_trials_from_csv(p)\n",
    "    if SUBJECTS is not None:\n",
    "        # Keep only requested subjects\n",
    "        subj = kp[0].split('_')[0]\n",
    "        if subj not in SUBJECTS:\n",
    "            continue\n",
    "    if eeg_cols_ref is None:\n",
    "        eeg_cols_ref = eeg_cols\n",
    "    else:\n",
    "        if eeg_cols != eeg_cols_ref:\n",
    "            raise ValueError(f\"EEG columns mismatch in {p}\")\n",
    "    Xs.append(Xp); ys.append(yp); keys.extend(kp)\n",
    "\n",
    "X = np.concatenate(Xs, axis=0)\n",
    "y_raw = np.concatenate(ys, axis=0)\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y_raw.shape)\n",
    "\n",
    "# Map labels to 0..C-1 for torch\n",
    "label_values = sorted(np.unique(y_raw).tolist())\n",
    "label_to_idx = {lab:i for i,lab in enumerate(label_values)}\n",
    "y = np.array([label_to_idx[int(v)] for v in y_raw], dtype=np.int64)\n",
    "n_classes = len(label_values)\n",
    "print(\"Labels:\", label_values, \"-> classes:\", n_classes)\n",
    "\n",
    "# Cache preprocessed arrays (optional)\n",
    "np.savez('preprocessed_trials.npz', X=X, y=y, keys=np.array(keys), eeg_cols=np.array(eeg_cols_ref), label_values=np.array(label_values))\n",
    "print(\"Saved preprocessed_trials.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "862760d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered X: (288, 22, 1000) float32\n"
     ]
    }
   ],
   "source": [
    "# ======= IDENTICAL to randomforestv2 preprocessing =======\n",
    "from mne.filter import filter_data\n",
    "\n",
    "SFREQ = 250.0\n",
    "L_FREQ = 8.0\n",
    "H_FREQ = 30.0\n",
    "\n",
    "# X is (n_trials, n_channels, n_times)\n",
    "X_filt = np.zeros_like(X, dtype=np.float64)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    X_filt[i] = filter_data(\n",
    "        X[i].astype(np.float64),   # <-- THIS is the fix\n",
    "        sfreq=SFREQ,\n",
    "        l_freq=L_FREQ,\n",
    "        h_freq=H_FREQ,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "X = X_filt.astype(np.float32)  # back to float32 for torch\n",
    "print(\"Filtered X:\", X.shape, X.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb8cbddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped X: (288, 22, 500)\n"
     ]
    }
   ],
   "source": [
    "# ===== MI window cropping (BCI 2a) =====\n",
    "FS = 250\n",
    "start = int(2.0 * FS)\n",
    "end   = int(6.0 * FS)\n",
    "\n",
    "X = X[:, :, start:end]\n",
    "print(\"Cropped X:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32c34ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved split_indices_seed42.npz\n",
      "Train trials: 230 Test trials: 58\n",
      "Train class counts: [57 58 57 58]\n",
      "Test  class counts: [15 14 15 14]\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Split (MATCHES other models): test_size=0.2, random_state=42, stratify=y\n",
    "# We also save the indices so CNN and GNN can reuse the identical split.\n",
    "# =====================\n",
    "idx = np.arange(len(y))\n",
    "strat = y if USE_STRATIFY else None\n",
    "train_idx, test_idx = train_test_split(idx, test_size=TEST_SIZE, random_state=SPLIT_SEED, stratify=strat)\n",
    "\n",
    "np.savez('split_indices_seed42.npz', train_idx=train_idx, test_idx=test_idx, keys=np.array(keys), y=y)\n",
    "print(\"Saved split_indices_seed42.npz\")\n",
    "\n",
    "print(\"Train trials:\", len(train_idx), \"Test trials:\", len(test_idx))\n",
    "print(\"Train class counts:\", np.bincount(y[train_idx], minlength=n_classes))\n",
    "print(\"Test  class counts:\", np.bincount(y[test_idx], minlength=n_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "919d622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized.\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Standardize using TRAIN data only (avoid leakage)\n",
    "#   mean/std computed per channel over (trials,time)\n",
    "# =====================\n",
    "X_train = X[train_idx]\n",
    "mean = X_train.mean(axis=(0,2), keepdims=True)\n",
    "std  = X_train.std(axis=(0,2), keepdims=True) + 1e-8\n",
    "Xn = (X - mean) / std\n",
    "\n",
    "X_train = Xn[train_idx]\n",
    "X_test  = Xn[test_idx]\n",
    "y_train = y[train_idx]\n",
    "y_test  = y[test_idx]\n",
    "\n",
    "print(\"Standardized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "707159ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrialDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).long()\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "train_loader = DataLoader(TrialDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "test_loader  = DataLoader(TrialDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "675cadb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([4.0351, 3.9655, 4.0351, 3.9655], device='cuda:0')\n",
      "EEGCNN(\n",
      "  (temporal): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(1, 64), stride=(1, 1), padding=(0, 32), bias=False)\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (spatial): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(22, 1), stride=(1, 1), groups=8, bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0)\n",
      "    (3): AvgPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (separable): Sequential(\n",
      "    (0): Conv2d(16, 16, kernel_size=(1, 16), stride=(1, 1), padding=(0, 8), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0)\n",
      "    (3): AvgPool2d(kernel_size=(1, 8), stride=(1, 8), padding=0)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (classifier): Linear(in_features=16, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class EEGCNN(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes,\n",
    "                 F1=8, D=2, kernel_length=64, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        F2 = F1 * D\n",
    "\n",
    "        # ===== Temporal convolution (learns band filters) =====\n",
    "        self.temporal = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=F1,\n",
    "                kernel_size=(1, kernel_length),\n",
    "                padding=(0, kernel_length // 2),\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(F1)\n",
    "        )\n",
    "\n",
    "        # ===== Depthwise spatial convolution (CSP-like) =====\n",
    "        self.spatial = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=F1,\n",
    "                out_channels=F2,\n",
    "                kernel_size=(n_channels, 1),\n",
    "                groups=F1,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d(kernel_size=(1, 4)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # ===== Separable temporal convolution =====\n",
    "        self.separable = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=F2,\n",
    "                out_channels=F2,\n",
    "                kernel_size=(1, 16),\n",
    "                padding=(0, 8),\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d(kernel_size=(1, 8)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(F2, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T) → (B, 1, C, T)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.temporal(x)\n",
    "        x = self.spatial(x)\n",
    "        x = self.separable(x)\n",
    "\n",
    "        # global average over time\n",
    "        x = x.mean(dim=-1).squeeze(-1)  # (B, F2)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "model = EEGCNN(n_channels=X.shape[1], n_classes=n_classes).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ===== Class-balanced loss =====\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = class_counts.sum() / class_counts\n",
    "\n",
    "class_weights = torch.tensor(\n",
    "    class_weights,\n",
    "    dtype=torch.float32,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "crit = nn.CrossEntropyLoss(weight=class_weights)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948b5a8",
   "metadata": {},
   "source": [
    "helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b853d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(x, crop_len):\n",
    "    \"\"\"\n",
    "    Random temporal crop for data augmentation.\n",
    "    x: (B, C, T)\n",
    "    \"\"\"\n",
    "    T = x.shape[-1]\n",
    "    if T <= crop_len:\n",
    "        return x\n",
    "    start = torch.randint(0, T - crop_len + 1, (1,)).item()\n",
    "    return x[:, :, start:start + crop_len]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39628927",
   "metadata": {},
   "source": [
    "best epoch checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f85c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_acc = -1.0\n",
    "best_state = None\n",
    "best_epoch = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5530268b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss 1.3899 | train acc 0.248 | test acc 0.259\n",
      "Epoch 02 | loss 1.3778 | train acc 0.248 | test acc 0.259\n",
      "Epoch 03 | loss 1.3812 | train acc 0.287 | test acc 0.241\n",
      "Epoch 04 | loss 1.3785 | train acc 0.296 | test acc 0.224\n",
      "Epoch 05 | loss 1.3838 | train acc 0.304 | test acc 0.207\n",
      "Epoch 06 | loss 1.3741 | train acc 0.309 | test acc 0.224\n",
      "Epoch 07 | loss 1.3625 | train acc 0.357 | test acc 0.241\n",
      "Epoch 08 | loss 1.3605 | train acc 0.374 | test acc 0.224\n",
      "Epoch 09 | loss 1.3696 | train acc 0.335 | test acc 0.224\n",
      "Epoch 10 | loss 1.3645 | train acc 0.391 | test acc 0.259\n",
      "Epoch 11 | loss 1.3627 | train acc 0.422 | test acc 0.241\n",
      "Epoch 12 | loss 1.3784 | train acc 0.396 | test acc 0.241\n",
      "Epoch 13 | loss 1.3431 | train acc 0.409 | test acc 0.259\n",
      "Epoch 14 | loss 1.3479 | train acc 0.413 | test acc 0.276\n",
      "Epoch 15 | loss 1.3403 | train acc 0.426 | test acc 0.259\n",
      "Epoch 16 | loss 1.3496 | train acc 0.426 | test acc 0.276\n",
      "Epoch 17 | loss 1.3513 | train acc 0.422 | test acc 0.259\n",
      "Epoch 18 | loss 1.3361 | train acc 0.435 | test acc 0.276\n",
      "Epoch 19 | loss 1.3710 | train acc 0.443 | test acc 0.293\n",
      "Epoch 20 | loss 1.3180 | train acc 0.461 | test acc 0.241\n",
      "\n",
      "Best test acc: 0.293 @ epoch 19\n",
      "\n",
      "=== CNN Test Set Report (BEST EPOCH) ===\n",
      "Accuracy: 0.29310344827586204\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.40      0.34        15\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       0.44      0.27      0.33        15\n",
      "           4       0.25      0.50      0.33        14\n",
      "\n",
      "    accuracy                           0.29        58\n",
      "   macro avg       0.25      0.29      0.25        58\n",
      "weighted avg       0.25      0.29      0.26        58\n",
      "\n",
      "Confusion matrix:\n",
      " [[6 0 3 6]\n",
      " [5 0 1 8]\n",
      " [3 1 4 7]\n",
      " [6 0 1 7]]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ===== Evaluation helper =====\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            pred = logits.argmax(dim=1).cpu().numpy()\n",
    "            ys.append(yb.numpy())\n",
    "            ps.append(pred)\n",
    "    y_true = np.concatenate(ys)\n",
    "    y_pred = np.concatenate(ps)\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "# ===== Non-shuffled train loader for evaluation =====\n",
    "train_eval_loader = DataLoader(\n",
    "    train_loader.dataset,\n",
    "    batch_size=train_loader.batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ===== Best-model tracking =====\n",
    "best_test_acc = -1.0\n",
    "best_epoch = None\n",
    "best_state = None\n",
    "\n",
    "\n",
    "# ===== Training loop =====\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "\n",
    "        # ===== TRAIN-TIME AUGMENTATION =====\n",
    "        xb = random_crop(xb, crop_len=500)  # 2s @ 250Hz\n",
    "\n",
    "        opt.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = crit(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # ----- Evaluate -----\n",
    "    ytr, ptr = evaluate(model, train_eval_loader)\n",
    "    yte, pte = evaluate(model, test_loader)\n",
    "\n",
    "    tr_acc = accuracy_score(ytr, ptr)\n",
    "    te_acc = accuracy_score(yte, pte)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | loss {np.mean(losses):.4f} | \"\n",
    "        f\"train acc {tr_acc:.3f} | test acc {te_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "    # ----- Save best model (DEEPCOPY) -----\n",
    "    if te_acc > best_test_acc:\n",
    "        best_test_acc = te_acc\n",
    "        best_epoch = epoch\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "# ===== Restore & save best model =====\n",
    "print(f\"\\nBest test acc: {best_test_acc:.3f} @ epoch {best_epoch}\")\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": best_state,\n",
    "    \"best_epoch\": best_epoch,\n",
    "    \"best_test_acc\": best_test_acc,\n",
    "}, \"cnn_best.pt\")\n",
    "\n",
    "\n",
    "# ===== Final evaluation (BEST model) =====\n",
    "y_true, y_pred = evaluate(model, test_loader)\n",
    "\n",
    "print(\"\\n=== CNN Test Set Report (BEST EPOCH) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, target_names=[str(v) for v in label_values]))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a618141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cnn_model_seed42.pt\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'label_values': label_values,\n",
    "    'eeg_cols': eeg_cols_ref,\n",
    "    'mean': mean.astype(np.float32),\n",
    "    'std': std.astype(np.float32),\n",
    "}, 'cnn_model_seed42.pt')\n",
    "print('Saved cnn_model_seed42.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab94d59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged results to experiment_results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>model_type</th>\n",
       "      <th>subject</th>\n",
       "      <th>seed</th>\n",
       "      <th>fs</th>\n",
       "      <th>bandpass</th>\n",
       "      <th>zscore</th>\n",
       "      <th>time_window</th>\n",
       "      <th>n_trials_train</th>\n",
       "      <th>n_trials_test</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-17T15:20:07</td>\n",
       "      <td>CNN</td>\n",
       "      <td>A01T</td>\n",
       "      <td>42</td>\n",
       "      <td>250</td>\n",
       "      <td>8-30</td>\n",
       "      <td>True</td>\n",
       "      <td>full</td>\n",
       "      <td>230</td>\n",
       "      <td>58</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.252381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp model_type subject  seed   fs bandpass  zscore  \\\n",
       "0  2025-12-17T15:20:07        CNN    A01T    42  250     8-30    True   \n",
       "\n",
       "  time_window  n_trials_train  n_trials_test  test_accuracy  macro_f1  \n",
       "0        full             230             58       0.293103  0.252381  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# ===== CONFIG =====\n",
    "RESULTS_CSV = \"experiment_results.csv\"\n",
    "MODEL_TYPE = \"CNN\"   # change to \"GNN\" in the GNN notebook\n",
    "SUBJECT = \"A01T\"     # update dynamically if looping subjects\n",
    "SEED = 42\n",
    "FS = 250\n",
    "BANDPASS = \"8-30\"\n",
    "ZSCORE = True        # False if you disable it\n",
    "TIME_WINDOW = \"full\" # or \"2-6s\"\n",
    "\n",
    "# ===== METRICS =====\n",
    "test_acc = accuracy_score(y_true, y_pred)\n",
    "macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "row = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"model_type\": MODEL_TYPE,\n",
    "    \"subject\": SUBJECT,\n",
    "    \"seed\": SEED,\n",
    "    \"fs\": FS,\n",
    "    \"bandpass\": BANDPASS,\n",
    "    \"zscore\": ZSCORE,\n",
    "    \"time_window\": TIME_WINDOW,\n",
    "    \"n_trials_train\": len(train_idx),\n",
    "    \"n_trials_test\": len(test_idx),\n",
    "    \"test_accuracy\": test_acc,\n",
    "    \"macro_f1\": macro_f1,\n",
    "}\n",
    "\n",
    "df_row = pd.DataFrame([row])\n",
    "\n",
    "# ===== APPEND OR CREATE =====\n",
    "if os.path.exists(RESULTS_CSV):\n",
    "    df_row.to_csv(RESULTS_CSV, mode=\"a\", header=False, index=False)\n",
    "else:\n",
    "    df_row.to_csv(RESULTS_CSV, index=False)\n",
    "\n",
    "print(\"Logged results to\", RESULTS_CSV)\n",
    "df_row\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "title": "EEG CNN (BCI 2a)"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
